---
title: "Homework 4: Machine Learning"
author: "Edward Davies"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false
options(scipen = 999) #disable scientific notation
library(tidyverse)
library(tidymodels)
library(GGally)
library(sf)
library(leaflet)
library(janitor)
library(rpart.plot)
library(here)
library(scales)
library(vip)
library(reshape)
library(Cubist)
library(C50)
library(kknn)
library(igraph)
```


# The Bechdel Test

https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/

The [Bechdel test](https://bechdeltest.com) is a way to assess how women are depicted in Hollywood movies.  In order for a movie to pass the test:

1. It has to have at least two [named] women in it
2. Who talk to each other
3. About something besides a man

There is a nice article and analysis you can find here https://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/
We have a sample of 1394 movies and we want to fit a model to predict whether a film passes the test or not.

```{r read_data}

bechdel <- read_csv(here::here("data", "bechdel.csv")) %>% 
  mutate(test = factor(test)) 
glimpse(bechdel)

```
How many films fail/pass the test, both as a number and as a %?

```{r}
# Assuming the column with pass/fail information is 'test' and pass is represented as 'Pass'
pass_fail_counts <- bechdel %>% 
  count(test)

# To calculate the percentage
pass_fail_counts <- pass_fail_counts %>%
  mutate(percent = n/sum(n) * 100)

# Display the counts and percentages
pass_fail_counts
```


## Movie scores
```{r}
ggplot(data = bechdel, aes(
  x = metascore,
  y = imdb_rating,
  colour = test
)) +
  geom_point(alpha = .3, size = 3) +
  scale_colour_manual(values = c("tomato", "olivedrab")) +
  labs(
    x = "Metacritic score",
    y = "IMDB rating",
    colour = "Bechdel test"
  ) +
 theme_light()
```


# Split the data
```{r}
# **Split the data**

set.seed(123)

data_split <- initial_split(bechdel, # updated data
                           prop = 0.8, 
                           strata = test)

bechdel_train <- training(data_split) 
bechdel_test <- testing(data_split)
```

Check the counts and % (proportions) of the `test` variable in each set.
```{r}
# For the training set
train_counts <- bechdel_train %>% 
  count(test) %>% 
  mutate(percent = n/sum(n) * 100)

# For the test set
test_counts <- bechdel_test %>% 
  count(test) %>% 
  mutate(percent = n/sum(n) * 100)

# Display the counts and percentages
train_counts
test_counts
```

## Feature exploration

```{r}
# Perform summary statistics for each of the features in the training data
summary(bechdel_train)

```

Grouped Summary Statistics:
```{r}
grouped_data <- bechdel_train %>% group_by(test) %>% summarise(mean_budget = mean(budget_2013), mean_domgross = mean(domgross_2013), mean_intgross = mean(intgross_2013), mean_metascore = mean(metascore), mean_imdb_rating = mean(imdb_rating))
print(grouped_data)
```


mean_budget: On average, films that fail the Bechdel test have a higher budget (approximately 6.85 in 2013 dollars) compared to films that pass the test (approximately 4.67 in 2013 dollars). This could suggest that films with larger budgets are less likely to have substantial female roles or female-centric narratives.

mean_domgross: Films that fail the Bechdel test also, on average, earn more at the domestic box office (around 10.65 in 2013 dollars) compared to films that pass the test (around 8.09 in 2013 dollars). This might indicate that films with less focus on female characters or narratives have historically performed better in domestic markets, although the difference is not substantial.

mean_intgross: Similar to domestic gross, films that fail the Bechdel test tend to earn more in international box office (approximately 23.25 in 2013 dollars) compared to films that pass the test (approximately 17.40 in 2013 dollars). This suggests that, like domestic markets, international audiences have also tended to favor films that do not pass the Bechdel test.

mean_metascore: Films that fail the Bechdel test have a slightly higher average Metascore (around 59.14) compared to films that pass the test (around 57.80). This might indicate that critics, on average, slightly favor films that fail the Bechdel test. However, the difference is minimal, indicating that whether a film passes or fails the Bechdel test does not significantly impact its critical reception.

mean_imdb_rating: Similarly, films that fail the Bechdel test have a slightly higher average IMDb rating (around 6.92) compared to films that pass the test (around 6.61). This might suggest that viewers have slightly favored films that fail the Bechdel test, although, like Metascore, the difference is not significant.


Proportions of Rated Groups:
```{r}
bechdel_train %>%
  group_by(rated) %>%
  summarise(n = n(), prop = n() / nrow(bechdel_train)) %>%
  ggplot(aes(x = reorder(rated, -prop), y = prop)) +
  geom_col() +
  labs(x = "Rated", y = "Proportion")
```

This bar graph shows how data is distributed across the different rated groups within the dataset. There are many R and PG rated films, with very few NC-17 films. This makes sense as NC-17 are very rare.

## Any outliers? 

```{r}


bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore) %>% 

    pivot_longer(cols = 2:6,
               names_to = "feature",
               values_to = "value") %>% 
  ggplot()+
  aes(x=test, y = value, fill = test)+
  coord_flip()+
  geom_boxplot()+
  facet_wrap(~feature, scales = "free")+
  theme_bw()+
  theme(legend.position = "none")+
  labs(x=NULL,y = NULL)

```
For budget, domestic gross and international gross, there are multiple outliers on the upper end but not the lower. This makes sense as although most films range within the 7 figures for budget, there are some "blockbusters" that have far higher budgets, and therefore generate far more revenue. Franchise films such as the Avengers or Fast & Furious fall under this category.

The rating measures, including the IMDB and Metascore rating, are far more consistent apart from a smaller number of outliers on the lower end, particularly for IMDB. This is likely due to reviews on either end of the spectrum being more common, with harsh reviews tending the be on the far lower end. The reason as the why the IMDB score contains more negative outliers is due to it being based on both critic and audience reviews (which tend to be harsher), whilst Metascore only contains critic scores.

## Scatterplot - Correlation Matrix

Write a paragraph discussing the output of the following 
```{r, warning=FALSE, message=FALSE}
bechdel %>% 
  select(test, budget_2013, domgross_2013, intgross_2013, imdb_rating, metascore)%>% 
  ggpairs(aes(colour=test), alpha=0.2)+
  theme_bw()
```
The correlation between imdb_rating and metascore for Fail group is 0.739, and for Pass it's 0.743. These are strong positive correlations, suggesting that movies with higher IMDB ratings also tend to have higher Metascore, regardless of whether they pass or fail the test. The correlation is almost the same for both groups, suggesting the relationship between IMDB rating and Metascore is similarly strong for both movies that pass and fail the Bechdel test.

The strong correlation between domestic gross and international gross for both Fail (0.927) and Pass (0.956) groups suggests that films that perform well domestically also tend to perform well internationally, regardless of whether they pass or fail the Bechdel test. This could imply that factors influencing domestic success also translate to international markets, such as genre, star power, or the film's overall quality. This relationship is slightly stronger for films that pass the Bechdel test.

On the other hand, the low correlation between the budget and both IMDb rating and Metascore suggests that a higher budget does not necessarily result in better critical ratings. The quality of a film, as assessed by both viewers (IMDb rating) and critics (Metascore), does not seem to be directly related to its production budget, whether it passes or fails the Bechdel test. This could be because film quality is determined by many factors beyond budget, such as script, direction, performances, etc. Therefore, spending more money on a film's production does not guarantee it will be better received by audiences or critics.

## Categorical variables

Write a paragraph discussing the output of the following 
```{r}
bechdel %>% 
  group_by(genre, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
  
 
bechdel %>% 
  group_by(rated, test) %>%
  summarise(n = n()) %>% 
  mutate(prop = n/sum(n))
```

Looking at the ratings, it seems that films rated G have approximately a 61.54% chance of failing the Bechdel test, and a 38.46% chance of passing. Films rated NC-17 have a much higher chance of failing the Bechdel test (83.33%) and a very low chance of passing (16.67%). PG-rated films are more evenly distributed with a failure rate of 56.10% and a pass rate of 43.90%. This is similar for PG-13 rated films, which have a failure rate of 52.90% and a pass rate of 47.10%. R-rated films have a higher chance of failing the Bechdel test (56.75%) than passing it (43.25%).

These findings suggest that films with more restricted ratings (NC-17 and R) are more likely to fail the Bechdel test. Conversely, films with more general audience ratings (G, PG, and PG-13) are somewhat more balanced, although they still fail the test more often than not. This could be due to a variety of factors, including differences in storytelling and character focus based on target audience demographics, or perhaps more mature or complex themes and narratives in R and NC-17 films that don't lend themselves as readily to passing the Bechdel test.

# Train first models. `test ~ metascore + imdb_rating`

```{r}
lr_mod <- logistic_reg() %>% 
  set_engine(engine = "glm") %>% 
  set_mode("classification")

lr_mod


tree_mod <- decision_tree() %>% 
  set_engine(engine = "C5.0") %>% 
  set_mode("classification")

tree_mod 
```

```{r}


lr_fit <- lr_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )

tree_fit <- tree_mod %>% # parsnip model
  fit(test ~ metascore + imdb_rating, # a formula
    data = bechdel_train # dataframe
  )
```

## Logistic regression

```{r}
lr_fit %>%
  broom::tidy()

lr_preds <- lr_fit %>%
  augment(new_data = bechdel_train) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0))

```

### Confusion matrix

```{r}
lr_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")


```
This model only returns a 58% accuracy rate, and therefore is not very reliable. 

## Decision Tree
```{r}
tree_preds <- tree_fit %>%
  augment(new_data = bechdel) %>%
  mutate(.pred_match = if_else(test == .pred_class, 1, 0)) 


```

```{r}
tree_preds %>% 
  conf_mat(truth = test, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```
This model has a very similar, but albeit slightly smaller accuracy rate, at just under 58%.

## Draw the decision tree

```{r}
draw_tree <- 
    rpart::rpart(
        test ~ metascore + imdb_rating,
        data = bechdel_train, # uses data that contains both birth weight and `low`
        control = rpart::rpart.control(maxdepth = 5, cp = 0, minsplit = 10)
    ) %>% 
    partykit::as.party()
plot(draw_tree)

```

# Cross Validation

Run the code below. What does it return?

```{r}
set.seed(123)
bechdel_folds <- vfold_cv(data = bechdel_train, 
                          v = 10, 
                          strata = test)
bechdel_folds
```

This code creates a 10-fold cross-validation plan with the bechdel_train data, with the folds being created in a stratified manner according to the test variable. This means that each fold will have roughly the same proportion of pass/fail as the original data. The output will be a tibble with two columns: .folds and .id. Each row in the .folds column is a list-column that contains the split objects for the resamples.

## `fit_resamples()`

Trains and tests a resampled model.

```{r}
lr_fit <- lr_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )


tree_fit <- tree_mod %>%
  fit_resamples(
    test ~ metascore + imdb_rating,
    resamples = bechdel_folds
  )
```


## `collect_metrics()`

Unnest the metrics column from a tidymodels `fit_resamples()`
```{r}

collect_metrics(lr_fit)
collect_metrics(tree_fit)


```


```{r}
tree_preds <- tree_mod %>% 
  fit_resamples(
    test ~ metascore + imdb_rating, 
    resamples = bechdel_folds,
    control = control_resamples(save_pred = TRUE) #<<
  )

# What does the data for ROC look like?
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail)  

# Draw the ROC
tree_preds %>% 
  collect_predictions() %>% 
  roc_curve(truth = test, .pred_Fail) %>% 
  autoplot()

```


# Build a better training set with `recipes`

## Preprocessing options

- Encode categorical predictors
- Center and scale variables
- Handle class imbalance
- Impute missing data
- Perform dimensionality reduction 
- ... ...

## To build a recipe

1. Start the `recipe()`
1. Define the variables involved
1. Describe **prep**rocessing [step-by-step]

## Collapse Some Categorical Levels

Do we have any `genre` with few observations?  Assign genres that have less than 3% to a new category 'Other'


```{r}
#| echo = FALSE
bechdel %>% 
  count(genre) %>% 
  mutate(genre = fct_reorder(genre, n)) %>% 
  ggplot(aes(x = genre, 
             y = n)) +
  geom_col(alpha = .8) +
  coord_flip() +
  labs(x = NULL) +
  geom_hline(yintercept = (nrow(bechdel_train)*.03), lty = 3)+
  theme_light()
```


```{r}
movie_rec <-
  recipe(test ~ .,
         data = bechdel_train) %>%
  
  # Genres with less than 5% will be in a catewgory 'Other'
    step_other(genre, threshold = .03) 
```
  

## Before recipe

```{r}
#| echo = FALSE
bechdel_train %>% 
  count(genre, sort = TRUE)
```


## After recipe

```{r}
movie_rec %>% 
  prep() %>% 
  bake(new_data = bechdel_train) %>% 
  count(genre, sort = TRUE)
```

## `step_dummy()`

Converts nominal data into numeric dummy variables

```{r}
#| results = "hide"
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_dummy(all_nominal_predictors()) 

movie_rec 
```

## Let's think about the modelling 

What if there were no films with `rated` NC-17 in the training data?

 - Will the model have a coefficient for rated NC-17?
No, the model will not have a coefficient for rated NC-17 if there were no films with this rating in the training data. Coefficients in a model represent the relationship between the predictors in the training data and the outcome variable. If a category does not exist in the training data, there is no way to estimate its effect, hence, it will not have a coefficient.

 - What will happen if the test data includes a film with rated NC-17?
If the test data includes a film with rated NC-17 and the training data does not have this category, the model will not be able to make an accurate prediction for this film. This is because the model has not learned any relationship between films with rated NC-17 and the outcome variable. The prediction may fail, or the model may treat this category as if it were the reference category or as missing data.

## `step_novel()`

Adds a catch-all level to a factor for any new values not encountered in model training, which lets R intelligently predict new levels in the test set.

```{r}

movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal_predictors) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal_predictors()) 

```


## `step_zv()`

Intelligently handles zero variance variables (variables that contain only a single value)

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes()) 
  
```


## `step_normalize()`

Centers then scales numeric variable (mean = 0, sd = 1)

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) 

```


## `step_corr()`

Removes highly correlated variables

```{r}
movie_rec <- recipe(test ~ ., data = bechdel) %>%
  step_other(genre, threshold = .03) %>% 
  step_novel(all_nominal(), -all_outcomes()) %>% # Use *before* `step_dummy()` so new level is dummified
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_numeric(), -all_outcomes())  %>% 
  step_normalize(all_numeric()) %>% 
  step_corr(all_predictors(), threshold = 0.75, method = "spearman") 



movie_rec
```


# Define different models to fit

```{r}
## Model Building

# 1. Pick a `model type`
# 2. set the `engine`
# 3. Set the `mode`: regression or classification

# Logistic regression
log_spec <-  logistic_reg() %>%  # model type
  set_engine(engine = "glm") %>%  # model engine
  set_mode("classification") # model mode

# Show your model specification
log_spec

# Decision Tree
tree_spec <- decision_tree() %>%
  set_engine(engine = "C5.0") %>%
  set_mode("classification")

tree_spec

# Random Forest
library(ranger)

rf_spec <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


# Boosted tree (XGBoost)
library(xgboost)

xgb_spec <- 
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification") 

# K-nearest neighbour (k-NN)
knn_spec <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 
```


# Bundle recipe and model with `workflows`


```{r}
log_wflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(movie_rec) %>%   # use the new recipe
 add_model(log_spec)   # add your model spec

# show object
log_wflow


## A few more workflows

tree_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(tree_spec) 

rf_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(rf_spec) 

xgb_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(xgb_spec)

knn_wflow <-
 workflow() %>%
 add_recipe(movie_rec) %>% 
 add_model(knn_spec)

```

HEADS UP

1. How many models have you specified?
2. What's the difference between a model specification and a workflow?
3. Do you need to add a formula (e.g., `test ~ .`)  if you have a recipe?


# Model Comparison

You now have all your models. Adapt the code from slides `code-from-slides-CA-housing.R`, line 400 onwards to assess which model gives you the best classification. 


```{r}
## Using `workflow()`, `fit()`, and `predict()`


# Fit the logistic regression workflow using the training data
log_fit <- log_wflow %>% 
  fit(data = bechdel_train)

# Make predictions using the logistic regression model on the test data
log_preds <- log_fit %>% 
  predict(new_data = bechdel_test) %>% 
  bind_cols(bechdel_test)


# View the predictions
head(log_preds)

# Repeat the process for the other models

# Decision Tree
tree_fit <- tree_wflow %>% 
  fit(data = bechdel_train)

tree_preds <- tree_fit %>% 
  predict(new_data = bechdel_test) %>% 
  bind_cols(bechdel_test)


# Random Forest
rf_fit <- rf_wflow %>% 
  fit(data = bechdel_train)

rf_preds <- rf_fit %>% 
  predict(new_data = bechdel_test) %>% 
  bind_cols(bechdel_test)

# Boosted Tree
xgb_fit <- xgb_wflow %>% 
  fit(data = bechdel_train)

xgb_preds <- xgb_fit %>% 
  predict(new_data = bechdel_test) %>% 
  bind_cols(bechdel_test)

# K-NN
knn_fit <- knn_wflow %>% 
  fit(data = bechdel_train)

knn_preds <- knn_fit %>% 
  predict(new_data = bechdel_test) %>% 
  bind_cols(bechdel_test)


# Let's see the classification metrics for all models

# Log Reg
log_metrics <- log_preds %>%
  metrics(truth = test, estimate = .pred_class)

# Decision Tree
tree_metrics <- tree_preds %>%
  metrics(truth = test, estimate = .pred_class)

# Random Forest
rf_metrics <- rf_preds %>%
  metrics(truth = test, estimate = .pred_class)

# Boosted Tree
xgb_metrics <- xgb_preds %>%
  metrics(truth = test, estimate = .pred_class)

# K-NN
knn_metrics <- knn_preds %>%
  metrics(truth = test, estimate = .pred_class)

# Combine all metrics into a single dataframe for easy comparison
combined_metrics <- bind_rows(
  log_metrics %>% add_column(Model = "Logistic Regression"),
  tree_metrics %>% add_column(Model = "Decision Tree"),
  rf_metrics %>% add_column(Model = "Random Forest"),
  xgb_metrics %>% add_column(Model = "Boosted Tree"),
  knn_metrics %>% add_column(Model = "K-NN")
)

# View the combined metrics
combined_metrics
```
#Logistic Regression:

Accuracy: 0.4321429. This means that the logistic regression model correctly predicted the class of the target variable approximately 43.21% of the time. This is generally considered low accuracy.
Kappa: -0.1052632. Kappa is an evaluation metric that takes into account the possibility of a correct prediction by chance. It's usually between 0 (random classification) and 1 (perfect classification). A negative value means the classifier is doing worse than random chance.

#Decision Tree:

Accuracy: 0.5892857. The decision tree model correctly predicted the class about 58.93% of the time. This is better than the logistic regression model.
Kappa: 0.1445271. The model's predictions are not random, but the value is still low, indicating a limited level of agreement.

#Random Forest:

Accuracy: 0.5785714. The random forest model had an accuracy of approximately 57.86%, which is slightly lower than the decision tree.
Kappa: 0.1158683. The kappa value is lower than that of the decision tree, suggesting that the model's predictions are not as reliable.

#Boosted Tree:

Accuracy: 0.5785714. The boosted tree model matched the random forest in accuracy, predicting correctly about 57.86% of the time.
Kappa: 0.1339450. Its kappa value is slightly higher than the random forest's, suggesting a better level of agreement.

#K-NN (K-Nearest Neighbors):

Accuracy: 0.5535714. The K-NN model predicted correctly about 55.36% of the time.
Kappa: 0.0000000. The kappa value of 0 indicates that this model's predictions are as good as random.


In summary, out of these models, the Decision Tree performed the best based on both accuracy and kappa metrics. Furthermore, these are not the only metrics to assess model performance, and the choice of metrics should be based on the specific use-case and problem at hand.


# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (Rmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
